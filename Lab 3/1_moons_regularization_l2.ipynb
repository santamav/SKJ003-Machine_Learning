{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ejemplo_lunas_regularizacion_l2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILYIlxhZVLTy",
        "colab_type": "text"
      },
      "source": [
        "**Fuente**:\n",
        "Jason Brownlee. *How to Use Weight Decay to Reduce Overfitting of Neural Network in Keras*. Machine Learning Mastery.\n",
        "URL: https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with-weight-regularization/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EQ9gIXlU4eC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mlp with weight regularization for the moons dataset plotting history\n",
        "from sklearn.datasets import make_moons\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from pandas import DataFrame\n",
        "from matplotlib import pyplot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIgdUiFFU_ut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate 2d classification dataset\n",
        "X, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n",
        "\n",
        "# scatter plot, dots colored by class value\n",
        "df = DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n",
        "colors = {0:'red', 1:'blue'}\n",
        "fig, ax = pyplot.subplots()\n",
        "grouped = df.groupby('label')\n",
        "for key, group in grouped:\n",
        "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyrKNKDcWePC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split into train and test\n",
        "n_train = 30\n",
        "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
        "trainy, testy = y[:n_train], y[n_train:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94JGp4nIc2rL",
        "colab_type": "text"
      },
      "source": [
        "Para ver el efecto del regularizador L2, descomenta el framento de 'kernel_regularizer', de forma que quede como sigue:\n",
        "\n",
        "*model.add(Dense(500, input_dim=2, activation='relu', kernel_regularizer=l2(0.001)))*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2sAYqK9VE-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(500, input_dim=2, activation='relu')) #, kernel_regularizer=l2(0.001)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model\n",
        "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=1500, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nZwndegVH2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot history\n",
        "# summarize history for accuracy\n",
        "pyplot.plot(history.history['accuracy'], label='train')\n",
        "pyplot.plot(history.history['val_accuracy'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}